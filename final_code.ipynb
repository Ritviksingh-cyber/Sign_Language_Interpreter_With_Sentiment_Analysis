{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import MobileNet, VGG16\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, class_name in enumerate(sentiment_folders):\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        print(f\"Class path: {class_path}\")\n",
    "        for image_file in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_file)\n",
    "            print(f\"Image path: {image_path}\")\n",
    "            try:\n",
    "                image = cv2.imread(image_path)\n",
    "                if image is None:\n",
    "                    print(f\"Error: Unable to read image at {image_path}\")\n",
    "                    continue\n",
    "\n",
    "                # Resize the image to 48x48 pixels\n",
    "                image = cv2.resize(image, (48, 48))\n",
    "                # Normalize the image\n",
    "                image = image / 255.0\n",
    "\n",
    "                # Append the image and corresponding label\n",
    "                x_data.append(image)\n",
    "                y_data.append(idx)  # Label index based on folder position\n",
    "                except cv2.error as e:\n",
    "                print(f\"Error: Unable to read image at {imagepath} due to formatting issues: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path, image_size=(48, 48)):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    # List of existing alphabet folders\n",
    "    alphabet_folders = [ch for ch in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' if ch not in ['H', 'J', 'Y']]\n",
    "\n",
    "    for idx, class_name in enumerate(alphabet_folders):\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        for image_file in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_file)\n",
    "            image = cv2.imread(image_path)\n",
    "\n",
    "            # Resize the image to 126x126 pixels\n",
    "            image = cv2.resize(image, image_size)\n",
    "            # Normalize the image\n",
    "            image = image / 255.0\n",
    "\n",
    "            # Append the image and corresponding label\n",
    "            x_data.append(image)\n",
    "            y_data.append(idx)  # Label index based on folder position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to numpy arrays\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "y_data = to_categorical(y_data, num_classes=len(alphabet_folders))\n",
    "\n",
    "# Usage:\n",
    "data_path = '/Users/divyash/Downloads/isl/ISL_Dataset'  # Update with the path to your dataset\n",
    "x_data, y_data, alphabet_folders = load_dataset(data_path)\n",
    "test_size = 0.1\n",
    "validation_size = 0.1\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_size, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=validation_size, random_state=42)\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output = Dense(23, activation='softmax')(x)  # 26 classes excluding H, J, Y\n",
    "\n",
    "model_sll = Model(inputs=base_model.input, outputs=output)\n",
    "# Compile the sign language recognition model\n",
    "model_sll.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the sign language recognition model\n",
    "model_sll.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the sign language recognition model on the test set\n",
    "test_loss, test_accuracy = model_sll.evaluate(x_test, y_test)\n",
    "print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")\n",
    "Initialize MediaPipe models\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Initialize the video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "# Initialize the hand and face models\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1)\n",
    "face_model = mp_face_mesh.FaceMesh(static_image_mode=False)\n",
    "face_results = face_model.process(frame_rgb)\n",
    "results = hands.process(frame_rgb)\n",
    "    \n",
    "if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Preprocess hand image for sign language recognition\n",
    "            frame_resized = cv2.resize(frame, (126, 126))\n",
    "            frame_normalized = frame_resized / 255.0\n",
    "            frame_normalized = np.expand_dims(frame_normalized, axis=0)\n",
    "# Extract face bounding box coordinates\n",
    "            face_coords = []\n",
    "            for landmark in face_landmarks.landmark:\n",
    "                x = min(max(int(landmark.x * frame.shape[1]), 0), frame.shape[1] - 1)\n",
    "                y = min(max(int(landmark.y * frame.shape[0]), 0), frame.shape[0] - 1)\n",
    "                face_coords.append((x, y))\n",
    "\n",
    "            # Calculate the bounding box of the face region\n",
    "            min_x = min([x for x, y in face_coords])\n",
    "            max_x = max([x for x, y in face_coords])\n",
    "            min_y = min([y for x, y in face_coords])\n",
    "            max_y = max([y for x, y in face_coords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the face region from the frame\n",
    "face_region = frame[min_y:max_y, min_x:max_x]\n",
    "\n",
    "            # Preprocess the face region for sentiment recognition\n",
    "face_region_resized = cv2.resize(face_region, (48, 48))\n",
    "face_region_normalized = face_region_resized / 255.0\n",
    "ace_region_normalized = np.expand_dims(face_region_normalized, axis=0)\n",
    "# Display the sentiment prediction on the frame\n",
    "cv2.putText(frame, f\"Sentiment: {predicted_sentiment_label}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow('MediaPipe Hand and Face Landmarks', frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
